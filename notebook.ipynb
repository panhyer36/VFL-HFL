{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cac029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥HFL_global_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from Model import TransformerModel\n",
    "device = torch.device(\"mps\")\n",
    "ssl_model_path = \"ssl_pretrain.pt\"\n",
    "hfl_model_path = \"HFL_global_model.pth\"\n",
    "HFL_Global_Model = TransformerModel(\n",
    "    feature_dim=14,\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    output_dim=1,\n",
    "    max_seq_length=100,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "SSL_Model = TransformerModel(\n",
    "    feature_dim=9,\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    output_dim=None,\n",
    "    max_seq_length=5000,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# è¼‰å…¥HFLæ¨¡å‹\n",
    "HFL_Global_Model.load_state_dict(torch.load(hfl_model_path, map_location=device))\n",
    "\n",
    "# è¼‰å…¥SSLæ¨¡å‹ - éœ€è¦å¾checkpointä¸­æå–model_state_dict\n",
    "ssl_checkpoint = torch.load(ssl_model_path, map_location=device)\n",
    "if isinstance(ssl_checkpoint, dict) and 'model_state_dict' in ssl_checkpoint:\n",
    "    SSL_Model.load_state_dict(ssl_checkpoint['model_state_dict'])\n",
    "else:\n",
    "    SSL_Model.load_state_dict(ssl_checkpoint)\n",
    "\n",
    "print(\"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "lepnw6uq1hk",
   "source": "# ä½¿ç”¨ Trainer è¼‰å…¥ Dataset\nimport glob\nimport os\nfrom config import load_config\nfrom DataLoader import SequenceCSVDataset\nfrom Trainer import FederatedTrainer\n\n# è¼‰å…¥é…ç½®ï¼ˆå¦‚æœæ²’æœ‰ config.yamlï¼Œéœ€è¦å…ˆå‰µå»ºä¸€å€‹ç°¡å–®çš„é…ç½®å°è±¡ï¼‰\ntry:\n    config = load_config('config.yaml')\nexcept:\n    # å¦‚æœæ²’æœ‰é…ç½®æ–‡ä»¶ï¼Œå‰µå»ºä¸€å€‹ç°¡å–®çš„é…ç½®å°è±¡\n    class SimpleConfig:\n        def __init__(self):\n            self.data_path = \"./data\"  # è«‹æ ¹æ“šå¯¦éš›è·¯å¾‘ä¿®æ”¹\n            self.input_length = 96\n            self.output_length = 1\n            self.features = ['feature1', 'feature2']  # è«‹æ ¹æ“šå¯¦éš›ç‰¹å¾µä¿®æ”¹\n            self.target = 'Power_Demand'\n            self.batch_size = 32\n            self.device = device\n    \n    config = SimpleConfig()\n\n# è¼‰å…¥æ•¸æ“šé›†ï¼ˆä»¥ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯ç‚ºä¾‹ï¼‰\ncsv_pattern = os.path.join(config.data_path, \"*.csv\")\ncsv_files = sorted(glob.glob(csv_pattern))\n\nif csv_files:\n    # è¼‰å…¥ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯çš„æ•¸æ“š\n    csv_file = csv_files[0]\n    csv_name = os.path.splitext(os.path.basename(csv_file))[0]\n    \n    print(f\"è¼‰å…¥å®¢æˆ¶ç«¯æ•¸æ“š: {csv_name}\")\n    \n    # å‰µå»ºæ•¸æ“šé›†å°è±¡\n    dataset = SequenceCSVDataset(\n        csv_path=config.data_path,\n        csv_name=csv_name,\n        input_len=config.input_length,\n        output_len=config.output_length,\n        features=config.features,\n        target=config.target,\n        save_path=config.data_path,\n        train_ratio=0.8,\n        val_ratio=0.1,\n        split_type='time_based',\n        fit_scalers=False  # ä½¿ç”¨å·²ä¿å­˜çš„æ¨™æº–åŒ–å™¨\n    )\n    \n    # ä½¿ç”¨ Trainer åˆ†å‰²æ•¸æ“šé›†\n    trainer = FederatedTrainer(HFL_Global_Model, config, device)\n    train_dataset, val_dataset, test_dataset = trainer.split_dataset(dataset)\n    \n    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨\n    train_loader, val_loader, test_loader = trainer.create_data_loaders(\n        train_dataset, val_dataset, test_dataset\n    )\n    \n    print(f\"\\nâœ“ æ•¸æ“šè¼‰å…¥æˆåŠŸï¼\")\n    print(f\"  - è¨“ç·´é›†å¤§å°: {len(train_dataset)} æ¨£æœ¬\")\n    print(f\"  - é©—è­‰é›†å¤§å°: {len(val_dataset)} æ¨£æœ¬\")\n    print(f\"  - æ¸¬è©¦é›†å¤§å°: {len(test_dataset)} æ¨£æœ¬\")\n    print(f\"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}\")\n    \n    # æŸ¥çœ‹ä¸€å€‹æ‰¹æ¬¡çš„æ•¸æ“šå½¢ç‹€\n    for inputs, targets in train_loader:\n        print(f\"\\næ•¸æ“šå½¢ç‹€:\")\n        print(f\"  - è¼¸å…¥: {inputs.shape}\")\n        print(f\"  - ç›®æ¨™: {targets.shape}\")\n        break\nelse:\n    print(f\"éŒ¯èª¤: åœ¨ {config.data_path} ç›®éŒ„ä¸‹æ²’æœ‰æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jodenz93nwc",
   "source": "# ä½¿ç”¨ Personalizer ç²å–æ¯å€‹å®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹\nfrom Personalizer import initialize_personalized_models, save_personalized_models\n\nprint(\"=\" * 70)\nprint(\"Per-FedAvg å€‹æ€§åŒ–æ¨¡å‹åˆå§‹åŒ–\")\nprint(\"=\" * 70)\n\n# æ–¹æ³• 1: ä½¿ç”¨é…ç½®æ–‡ä»¶é€²è¡Œå€‹æ€§åŒ–\n# ç¢ºä¿ config ä¸­åŒ…å«å¿…è¦çš„åƒæ•¸\nif not hasattr(config, 'adaptation_lr'):\n    config.adaptation_lr = 0.01  # å€‹æ€§åŒ–é©æ‡‰å­¸ç¿’ç‡\nif not hasattr(config, 'personalization_steps'):\n    config.personalization_steps = 10  # å€‹æ€§åŒ–é©æ‡‰æ­¥æ•¸\nif not hasattr(config, 'model_save_path'):\n    config.model_save_path = \"./models\"  # æ¨¡å‹ä¿å­˜è·¯å¾‘\n\n# æŒ‡å®šå…¨å±€æ¨¡å‹è·¯å¾‘ï¼ˆPer-FedAvg è¨“ç·´å¥½çš„æ¨¡å‹ï¼‰\nglobal_model_path = hfl_model_path  # ä½¿ç”¨å‰é¢è¼‰å…¥çš„ HFL æ¨¡å‹è·¯å¾‘\n\ntry:\n    # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹\n    print(f\"\\nä½¿ç”¨å…¨å±€æ¨¡å‹: {global_model_path}\")\n    print(f\"å€‹æ€§åŒ–åƒæ•¸:\")\n    print(f\"  - é©æ‡‰å­¸ç¿’ç‡: {config.adaptation_lr}\")\n    print(f\"  - é©æ‡‰æ­¥æ•¸: {config.personalization_steps}\")\n    print(f\"  - è¨­å‚™: {config.device}\")\n    \n    # ç²å–æ‰€æœ‰å®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹ç‹€æ…‹å­—å…¸\n    client_models = initialize_personalized_models(config, global_model_path)\n    \n    print(f\"\\nâœ“ æˆåŠŸç²å– {len(client_models)} å€‹å®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹\")\n    print(f\"\\nå®¢æˆ¶ç«¯åˆ—è¡¨:\")\n    for idx, client_name in enumerate(client_models.keys(), 1):\n        print(f\"  {idx}. {client_name}\")\n    \n    # å¯é¸ï¼šä¿å­˜å€‹æ€§åŒ–æ¨¡å‹åˆ°ç£ç›¤\n    save_dir = \"personalized_models\"\n    save_personalized_models(client_models, save_dir)\n    \n    # ç¤ºç¯„ï¼šå¦‚ä½•ä½¿ç”¨ç‰¹å®šå®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹\n    print(f\"\\n{'=' * 70}\")\n    print(\"ä½¿ç”¨ç¯„ä¾‹ï¼šè¼‰å…¥ç‰¹å®šå®¢æˆ¶ç«¯çš„å€‹æ€§åŒ–æ¨¡å‹\")\n    print(\"=\" * 70)\n    \n    if client_models:\n        # ç²å–ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯çš„åç¨±\n        first_client = list(client_models.keys())[0]\n        \n        print(f\"\\nç¤ºç¯„ï¼šè¼‰å…¥å®¢æˆ¶ç«¯ '{first_client}' çš„å€‹æ€§åŒ–æ¨¡å‹\")\n        \n        # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹\n        personalized_model_client_1 = TransformerModel(\n            feature_dim=config.feature_dim if hasattr(config, 'feature_dim') else 14,\n            d_model=256,\n            nhead=8,\n            num_layers=4,\n            output_dim=None,  # VFL å ´æ™¯ï¼šä¸éœ€è¦è¼¸å‡ºå±¤\n            max_seq_length=100,\n            dropout=0.1\n        ).to(device)\n        \n        # è¼‰å…¥å€‹æ€§åŒ–å¾Œçš„æ¬Šé‡\n        personalized_model_client_1.load_state_dict(client_models[first_client])\n        personalized_model_client_1.eval()\n        \n        print(f\"âœ“ æˆåŠŸè¼‰å…¥å®¢æˆ¶ç«¯ '{first_client}' çš„å€‹æ€§åŒ–æ¨¡å‹\")\n        print(f\"\\næ¨¡å‹å·²æº–å‚™å¥½ç”¨æ–¼ VFL è¨“ç·´ï¼\")\n        \n        # ç¤ºç¯„ï¼šä½¿ç”¨å€‹æ€§åŒ–æ¨¡å‹é€²è¡Œå‰å‘å‚³æ’­ï¼ˆä¸å«è¼¸å‡ºæŠ•å½±ï¼‰\n        print(f\"\\n{'=' * 70}\")\n        print(\"VFL å ´æ™¯ç¤ºç¯„ï¼šä½¿ç”¨å€‹æ€§åŒ–æ¨¡å‹ç”ŸæˆåµŒå…¥å‘é‡\")\n        print(\"=\" * 70)\n        \n        # æ¨¡æ“¬å…©å€‹åƒèˆ‡æ–¹çš„æ•¸æ“š\n        batch_size = 8\n        seq_length = 96\n        \n        # Party A çš„æ•¸æ“šï¼ˆå®¢æˆ¶ç«¯ 1ï¼‰\n        x_party_a = torch.randn(batch_size, seq_length, 14).to(device)\n        \n        with torch.no_grad():\n            # ä½¿ç”¨å€‹æ€§åŒ–æ¨¡å‹ç”ŸæˆåµŒå…¥ï¼ˆä¸å«è¼¸å‡ºæŠ•å½±ï¼‰\n            embedding_a = personalized_model_client_1.forward_embedding(x_party_a)\n        \n        print(f\"\\nâœ“ Party A åµŒå…¥å‘é‡å½¢ç‹€: {embedding_a.shape}\")\n        print(f\"  - æ‰¹æ¬¡å¤§å°: {embedding_a.shape[0]}\")\n        print(f\"  - åµŒå…¥ç¶­åº¦: {embedding_a.shape[1]}\")\n        print(f\"\\né€™å€‹åµŒå…¥å‘é‡å¯ä»¥ç™¼é€åˆ° Server é€²è¡Œèåˆé æ¸¬ï¼\")\n        print(f\"åŸå§‹æ•¸æ“š ({x_party_a.shape}) ä¿æŒåœ¨æœ¬åœ°ï¼Œä¿è­·éš±ç§ã€‚\")\n        \nexcept FileNotFoundError as e:\n    print(f\"\\nâŒ éŒ¯èª¤: {e}\")\n    print(\"è«‹ç¢ºèªå…¨å±€æ¨¡å‹è·¯å¾‘æ˜¯å¦æ­£ç¢º\")\nexcept Exception as e:\n    print(f\"\\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}\")\n    print(\"è«‹æª¢æŸ¥é…ç½®å’Œæ•¸æ“šè·¯å¾‘æ˜¯å¦æ­£ç¢º\")\n\nprint(f\"\\n{'=' * 70}\")\nprint(\"å€‹æ€§åŒ–æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "t67xhdxjs4n",
   "source": "# ============================================================================\n# VFL å ´æ™¯ï¼šè¼‰å…¥ Weather (é›²ç«¯) å’Œ HFL (æœ¬åœ°) çš„ Dataset\n# ============================================================================\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport numpy as np\nfrom DataLoader import SequenceCSVDataset\n\nprint(\"=\" * 80)\nprint(\"VFL æ•¸æ“šè¼‰å…¥ï¼šWeather (é›²ç«¯) + HFL (æœ¬åœ°)\")\nprint(\"=\" * 80)\n\n# === é…ç½®åƒæ•¸ ===\ndata_path = \"./data\"\nweather_csv_name = \"Weather\"  # ä¸å« .csv\nhfl_csv_name = \"processed/Consumer_01\"  # ä½¿ç”¨ç¬¬ä¸€å€‹æ¶ˆè²»è€…ä½œç‚ºç¤ºä¾‹\n\n# HFL ç‰¹å¾µï¼ˆæœ¬åœ°å®¢æˆ¶ç«¯ï¼‰\nhfl_features = [\n    'AC1', 'AC2', 'AC3', 'AC4', \n    'Dish washer', 'Washing Machine', 'Dryer', 'Water heater',\n    'TV', 'Microwave', 'Kettle', 'Lighting', 'Refrigerator', \n    'Consumption_Total'\n]\n\n# Weather ç‰¹å¾µï¼ˆé›²ç«¯ï¼‰\nweather_features = [\n    'TemperatureC', 'DewpointC', 'PressurehPa', \n    'WindSpeedKMH', 'WindSpeedGustKMH', 'Humidity',\n    'HourlyPrecipMM', 'dailyrainMM', 'SolarRadiationWatts_m2'\n]\n\n# ç›®æ¨™è®Šé‡\ntarget = ['Power_Demand']  # SequenceCSVDataset éœ€è¦åˆ—è¡¨æ ¼å¼\n\n# åºåˆ—åƒæ•¸\nseq_length = 96\noutput_length = 1\nbatch_size = 32\n\nprint(f\"\\nã€æ•¸æ“šé…ç½®ã€‘\")\nprint(f\"  - Weather ç‰¹å¾µæ•¸: {len(weather_features)}\")\nprint(f\"  - HFL ç‰¹å¾µæ•¸: {len(hfl_features)}\")\nprint(f\"  - åºåˆ—é•·åº¦: {seq_length}\")\nprint(f\"  - è¼¸å‡ºé•·åº¦: {output_length}\")\nprint(f\"  - æ‰¹æ¬¡å¤§å°: {batch_size}\")\n\n# ============================================================================\n# æ­¥é©Ÿ 1: ä½¿ç”¨ SequenceCSVDataset è¼‰å…¥ Weather æ•¸æ“šï¼ˆé›²ç«¯ï¼‰\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 1ã€‘è¼‰å…¥ Weather æ•¸æ“šï¼ˆé›²ç«¯ï¼Œä½¿ç”¨ SequenceCSVDatasetï¼‰\")\n\n# æª¢æŸ¥ Weather CSV æ˜¯å¦æœ‰ç›®æ¨™è®Šé‡\nweather_df_check = pd.read_csv(f\"{data_path}/{weather_csv_name}.csv\")\nprint(f\"  - Weather åŸå§‹æ•¸æ“šå½¢ç‹€: {weather_df_check.shape}\")\nprint(f\"  - Weather å¯ç”¨æ¬„ä½: {list(weather_df_check.columns)}\")\n\n# Weather æ•¸æ“šä½¿ç”¨å‡ç›®æ¨™ï¼ˆå› ç‚ºæ²’æœ‰çœŸå¯¦ç›®æ¨™ï¼‰\n# æˆ‘å€‘å°‡å‰µå»ºä¸€å€‹å‡çš„ç›®æ¨™æ¬„ä½ï¼Œä½†å¯¦éš›ä¸Šä¸æœƒä½¿ç”¨å®ƒ\nif 'Power_Demand' not in weather_df_check.columns:\n    print(f\"  âš  Weather æ²’æœ‰ Power_Demand æ¬„ä½ï¼Œå°‡ä½¿ç”¨ Weather ç‰¹å¾µå‰µå»ºæ•¸æ“šé›†\")\n    # ç”±æ–¼ SequenceCSVDataset éœ€è¦ç›®æ¨™æ¬„ä½ï¼Œæˆ‘å€‘éœ€è¦æ‰‹å‹•è™•ç† Weather æ•¸æ“š\n    \n    # æ‰‹å‹•è¼‰å…¥å’Œæ¨™æº–åŒ– Weather æ•¸æ“š\n    weather_data_raw = weather_df_check[weather_features].values\n    \n    # Weather æ¨™æº–åŒ–å™¨\n    weather_scaler_path = f\"{data_path}/weather_scaler.pkl\"\n    if os.path.exists(weather_scaler_path):\n        with open(weather_scaler_path, 'rb') as f:\n            weather_scaler = pickle.load(f)\n        print(f\"  âœ“ Weather æ¨™æº–åŒ–å™¨å·²è¼‰å…¥\")\n    else:\n        weather_scaler = StandardScaler()\n        weather_scaler.fit(weather_data_raw)\n        with open(weather_scaler_path, 'wb') as f:\n            pickle.dump(weather_scaler, f)\n        print(f\"  âœ“ Weather æ¨™æº–åŒ–å™¨å·²å‰µå»ºä¸¦ä¿å­˜\")\n    \n    weather_data_scaled = weather_scaler.transform(weather_data_raw)\n    print(f\"  âœ“ Weather æ•¸æ“šå·²æ¨™æº–åŒ–: {weather_data_scaled.shape}\")\n\n# ============================================================================\n# æ­¥é©Ÿ 2: ä½¿ç”¨ SequenceCSVDataset è¼‰å…¥ HFL æ•¸æ“šï¼ˆæœ¬åœ°ï¼‰\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 2ã€‘è¼‰å…¥ HFL æ•¸æ“šï¼ˆæœ¬åœ°ï¼Œä½¿ç”¨ SequenceCSVDatasetï¼‰\")\n\n# æª¢æŸ¥ HFL æ•¸æ“šæ˜¯å¦æœ‰ç›®æ¨™\nhfl_df_check = pd.read_csv(f\"{data_path}/{hfl_csv_name}.csv\")\nprint(f\"  - HFL åŸå§‹æ•¸æ“šå½¢ç‹€: {hfl_df_check.shape}\")\n\n# ç¢ºå®šç›®æ¨™è®Šé‡\nif 'Power_Demand' not in hfl_df_check.columns:\n    if 'Consumption_Total' in hfl_df_check.columns:\n        target = ['Consumption_Total']\n        print(f\"  âš  ä½¿ç”¨ 'Consumption_Total' ä½œç‚ºç›®æ¨™\")\n    else:\n        raise ValueError(\"HFL æ•¸æ“šä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ¨™è®Šé‡\")\n\n# ä½¿ç”¨ SequenceCSVDataset è¼‰å…¥ HFL æ•¸æ“š\ntry:\n    hfl_dataset = SequenceCSVDataset(\n        csv_path=data_path,\n        csv_name=hfl_csv_name,\n        input_len=seq_length,\n        output_len=output_length,\n        features=hfl_features,\n        target=target,\n        save_path=data_path,\n        train_ratio=0.8,\n        val_ratio=0.1,\n        split_type='time_based',\n        fit_scalers=True  # ç¬¬ä¸€æ¬¡é‹è¡Œæ™‚å‰µå»ºæ¨™æº–åŒ–å™¨\n    )\n    print(f\"  âœ“ HFL Dataset å·²å‰µå»º\")\n    print(f\"    - ç¸½åºåˆ—æ•¸: {len(hfl_dataset)}\")\n    \n    # ä¿å­˜æ¨™æº–åŒ–å™¨ä¾›å¾ŒçºŒä½¿ç”¨\n    hfl_scaler = hfl_dataset.feature_scaler\n    target_scaler = hfl_dataset.target_scaler\n    \n    # å°‡æ¨™æº–åŒ–å™¨ä¿å­˜åˆ°çµ±ä¸€ä½ç½®\n    with open(f\"{data_path}/hfl_scaler.pkl\", 'wb') as f:\n        pickle.dump(hfl_scaler, f)\n    with open(f\"{data_path}/target_scaler.pkl\", 'wb') as f:\n        pickle.dump(target_scaler, f)\n    print(f\"  âœ“ æ¨™æº–åŒ–å™¨å·²ä¿å­˜åˆ° {data_path}/\")\n    \nexcept FileNotFoundError as e:\n    print(f\"  âš  æ¨™æº–åŒ–å™¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨ fit_scalers=True å‰µå»º\")\n    hfl_dataset = SequenceCSVDataset(\n        csv_path=data_path,\n        csv_name=hfl_csv_name,\n        input_len=seq_length,\n        output_len=output_length,\n        features=hfl_features,\n        target=target,\n        save_path=data_path,\n        train_ratio=0.8,\n        val_ratio=0.1,\n        split_type='time_based',\n        fit_scalers=True\n    )\n    hfl_scaler = hfl_dataset.feature_scaler\n    target_scaler = hfl_dataset.target_scaler\n\n# ============================================================================\n# æ­¥é©Ÿ 3: å‰µå»ºåŒ¹é…çš„ Weather åºåˆ—æ•¸æ“š\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 3ã€‘å‰µå»ºåŒ¹é…çš„ Weather åºåˆ—æ•¸æ“š\")\n\n# å‰µå»ºèˆ‡ HFL å°æ‡‰çš„ Weather åºåˆ—\ndef create_weather_sequences(weather_data, seq_len, total_len):\n    \"\"\"å‰µå»º Weather æ™‚åºåºåˆ—\"\"\"\n    sequences = []\n    for i in range(min(len(weather_data) - seq_len + 1, total_len)):\n        sequences.append(weather_data[i:i+seq_len])\n    return np.array(sequences)\n\n# ç²å– HFL çš„ç¸½åºåˆ—æ•¸\ntotal_hfl_sequences = len(hfl_dataset)\nweather_sequences = create_weather_sequences(weather_data_scaled, seq_length, total_hfl_sequences)\n\nprint(f\"  âœ“ Weather åºåˆ—å·²å‰µå»º: {weather_sequences.shape}\")\nprint(f\"  âœ“ HFL åºåˆ—æ•¸é‡: {total_hfl_sequences}\")\n\n# ç¢ºä¿å…©è€…é•·åº¦åŒ¹é…\nmin_len = min(len(weather_sequences), total_hfl_sequences)\nweather_sequences = weather_sequences[:min_len]\nprint(f\"  âœ“ å°é½Šå¾Œåºåˆ—æ•¸é‡: {min_len}\")\n\n# ============================================================================\n# æ­¥é©Ÿ 4: ä½¿ç”¨ SequenceCSVDataset çš„åˆ†å‰²ç´¢å¼•\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 4ã€‘åˆ†å‰²è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†ï¼ˆä½¿ç”¨ SequenceCSVDataset çš„åˆ†å‰²ï¼‰\")\n\n# ç²å– HFL dataset çš„åˆ†å‰²ç´¢å¼•\ntrain_indices = hfl_dataset.train_indices\nval_indices = hfl_dataset.val_indices\ntest_indices = hfl_dataset.test_indices\n\nprint(f\"  âœ“ è¨“ç·´é›†ç´¢å¼•: {len(train_indices)} å€‹åºåˆ—\")\nprint(f\"  âœ“ é©—è­‰é›†ç´¢å¼•: {len(val_indices)} å€‹åºåˆ—\")\nprint(f\"  âœ“ æ¸¬è©¦é›†ç´¢å¼•: {len(test_indices)} å€‹åºåˆ—\")\n\n# åˆ†å‰² Weather åºåˆ—\nX_weather_train = weather_sequences[train_indices]\nX_weather_val = weather_sequences[val_indices]\nX_weather_test = weather_sequences[test_indices]\n\n# å¾ HFL dataset ç²å–æ•¸æ“š\ntrain_dataset = hfl_dataset.get_train_dataset()\nval_dataset = hfl_dataset.get_val_dataset()\ntest_dataset = hfl_dataset.get_test_dataset()\n\n# æå– HFL åºåˆ—å’Œç›®æ¨™\nX_hfl_train = np.array([hfl_dataset.input_seq[i] for i in train_indices])\nX_hfl_val = np.array([hfl_dataset.input_seq[i] for i in val_indices])\nX_hfl_test = np.array([hfl_dataset.input_seq[i] for i in test_indices])\n\ny_train = np.array([hfl_dataset.output_seq[i] for i in train_indices])\ny_val = np.array([hfl_dataset.output_seq[i] for i in val_indices])\ny_test = np.array([hfl_dataset.output_seq[i] for i in test_indices])\n\nprint(f\"\\n  ã€æ•¸æ“šå½¢ç‹€ã€‘\")\nprint(f\"    è¨“ç·´é›†:\")\nprint(f\"      - Weather: {X_weather_train.shape}\")\nprint(f\"      - HFL: {X_hfl_train.shape}\")\nprint(f\"      - Target: {y_train.shape}\")\nprint(f\"    é©—è­‰é›†:\")\nprint(f\"      - Weather: {X_weather_val.shape}\")\nprint(f\"      - HFL: {X_hfl_val.shape}\")\nprint(f\"      - Target: {y_val.shape}\")\nprint(f\"    æ¸¬è©¦é›†:\")\nprint(f\"      - Weather: {X_weather_test.shape}\")\nprint(f\"      - HFL: {X_hfl_test.shape}\")\nprint(f\"      - Target: {y_test.shape}\")\n\n# ============================================================================\n# æ­¥é©Ÿ 5: å‰µå»º PyTorch DataLoader\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 5ã€‘å‰µå»º PyTorch DataLoader\")\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# è½‰æ›ç‚º PyTorch Tensor\nX_weather_train_t = torch.FloatTensor(X_weather_train).to(device)\nX_hfl_train_t = torch.FloatTensor(X_hfl_train).to(device)\ny_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(device) if y_train.ndim == 1 else torch.FloatTensor(y_train).to(device)\n\nX_weather_val_t = torch.FloatTensor(X_weather_val).to(device)\nX_hfl_val_t = torch.FloatTensor(X_hfl_val).to(device)\ny_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(device) if y_val.ndim == 1 else torch.FloatTensor(y_val).to(device)\n\nX_weather_test_t = torch.FloatTensor(X_weather_test).to(device)\nX_hfl_test_t = torch.FloatTensor(X_hfl_test).to(device)\ny_test_t = torch.FloatTensor(y_test).unsqueeze(1).to(device) if y_test.ndim == 1 else torch.FloatTensor(y_test).to(device)\n\n# å‰µå»º Dataset\ntrain_dataset_vfl = TensorDataset(X_weather_train_t, X_hfl_train_t, y_train_t)\nval_dataset_vfl = TensorDataset(X_weather_val_t, X_hfl_val_t, y_val_t)\ntest_dataset_vfl = TensorDataset(X_weather_test_t, X_hfl_test_t, y_test_t)\n\n# å‰µå»º DataLoader\ntrain_loader_vfl = DataLoader(train_dataset_vfl, batch_size=batch_size, shuffle=True)\nval_loader_vfl = DataLoader(val_dataset_vfl, batch_size=batch_size, shuffle=False)\ntest_loader_vfl = DataLoader(test_dataset_vfl, batch_size=batch_size, shuffle=False)\n\nprint(f\"  âœ“ è¨“ç·´ DataLoader: {len(train_loader_vfl)} æ‰¹æ¬¡\")\nprint(f\"  âœ“ é©—è­‰ DataLoader: {len(val_loader_vfl)} æ‰¹æ¬¡\")\nprint(f\"  âœ“ æ¸¬è©¦ DataLoader: {len(test_loader_vfl)} æ‰¹æ¬¡\")\n\n# ä¿å­˜ create_sequences å‡½æ•¸ä¾›å¾ŒçºŒä½¿ç”¨\ndef create_sequences(weather, hfl, targets, seq_len):\n    \"\"\"å‰µå»ºæ™‚åºåºåˆ—æ•¸æ“šï¼ˆä¾›å…¶ä»–å®¢æˆ¶ç«¯ä½¿ç”¨ï¼‰\"\"\"\n    X_weather, X_hfl, y = [], [], []\n    \n    for i in range(len(weather) - seq_len):\n        X_weather.append(weather[i:i+seq_len])\n        X_hfl.append(hfl[i:i+seq_len])\n        y.append(targets[i+seq_len])\n    \n    return np.array(X_weather), np.array(X_hfl), np.array(y)\n\n# ============================================================================\n# æ•¸æ“šè¼‰å…¥å®Œæˆ\n# ============================================================================\nprint(f\"\\n{'=' * 80}\")\nprint(\"âœ“ VFL æ•¸æ“šè¼‰å…¥å®Œæˆï¼ˆä½¿ç”¨ SequenceCSVDatasetï¼‰ï¼\")\nprint(f\"{'=' * 80}\")\nprint(f\"\\næ•¸æ“šæ‘˜è¦:\")\nprint(f\"  ã€é›²ç«¯ - Weatherï¼ˆç„¡ç›®æ¨™ï¼‰ã€‘\")\nprint(f\"    - ç‰¹å¾µç¶­åº¦: {len(weather_features)}\")\nprint(f\"    - è¨“ç·´æ¨£æœ¬: {X_weather_train.shape}\")\nprint(f\"    - æ¨™æº–åŒ–å™¨: {weather_scaler_path}\")\nprint(f\"  ã€æœ¬åœ° - HFLï¼ˆæœ‰ç›®æ¨™ï¼‰ã€‘\")\nprint(f\"    - ç‰¹å¾µç¶­åº¦: {len(hfl_features)}\")\nprint(f\"    - è¨“ç·´æ¨£æœ¬: {X_hfl_train.shape}\")\nprint(f\"    - ä½¿ç”¨ SequenceCSVDataset ç®¡ç†\")\nprint(f\"  ã€ç›®æ¨™è®Šé‡ã€‘\")\nprint(f\"    - è®Šé‡: {target}\")\nprint(f\"    - è¨“ç·´æ¨£æœ¬: {y_train.shape}\")\nprint(f\"    - æ¨™æº–åŒ–å™¨: target_scaler (ä¾†è‡ª SequenceCSVDataset)\")\nprint(f\"\\nä¸‹ä¸€æ­¥ï¼šå»ºç«‹ Weather Modelï¼ˆé›²ç«¯ï¼‰å’Œ Fusion Modelï¼ˆæœ¬åœ°ï¼‰\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bkb7cu5skf",
   "source": "# ============================================================================\n# VFL è¯é‚¦å­¸ç¿’è¨“ç·´ï¼šå¤šå®¢æˆ¶ç«¯ + FedAvg + åˆ†éšæ®µè¨“ç·´\n# ============================================================================\n\nfrom Model import TransformerModel, FusionModel\nfrom tqdm import tqdm\nimport random\nimport copy\n\nprint(\"=\" * 80)\nprint(\"VFL è¯é‚¦å­¸ç¿’è¨“ç·´ï¼šå¤šå®¢æˆ¶ç«¯ FedAvg\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# æ­¥é©Ÿ 1: è¼‰å…¥æ‰€æœ‰å®¢æˆ¶ç«¯æ•¸æ“š\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 1ã€‘è¼‰å…¥æ‰€æœ‰å®¢æˆ¶ç«¯æ•¸æ“š\")\n\n# ç²å–æ‰€æœ‰å®¢æˆ¶ç«¯çš„ CSV æ–‡ä»¶\nimport glob\nclient_csv_files = sorted(glob.glob(f\"{data_path}/processed/Consumer_*.csv\"))\nprint(f\"  âœ“ æ‰¾åˆ° {len(client_csv_files)} å€‹å®¢æˆ¶ç«¯\")\n\n# ç‚ºæ¯å€‹å®¢æˆ¶ç«¯å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨\nclient_dataloaders = {}\nclient_names = []\n\nfor csv_file in client_csv_files:\n    client_name = os.path.basename(csv_file).replace('.csv', '')\n    client_names.append(client_name)\n    \n    # è¼‰å…¥å®¢æˆ¶ç«¯æ•¸æ“š\n    client_df = pd.read_csv(csv_file)\n    \n    # æª¢æŸ¥ç›®æ¨™è®Šé‡\n    if target not in client_df.columns:\n        client_target = 'Consumption_Total'\n    else:\n        client_target = target\n    \n    # æå–ç‰¹å¾µå’Œç›®æ¨™\n    client_hfl_data = client_df[hfl_features].values\n    client_target_data = client_df[client_target].values\n    \n    # æ¨™æº–åŒ–\n    client_hfl_scaled = hfl_scaler.transform(client_hfl_data)\n    client_target_scaled = target_scaler.transform(client_target_data.reshape(-1, 1)).flatten()\n    \n    # å°é½Šé•·åº¦\n    min_len = min(len(weather_data_scaled), len(client_hfl_scaled), len(client_target_scaled))\n    client_weather = weather_data_scaled[:min_len]\n    client_hfl = client_hfl_scaled[:min_len]\n    client_targets = client_target_scaled[:min_len]\n    \n    # å‰µå»ºåºåˆ—\n    X_w, X_h, y = create_sequences(client_weather, client_hfl, client_targets, seq_length)\n    \n    # åˆ†å‰²æ•¸æ“šé›†ï¼ˆ8:1:1ï¼‰\n    total = len(X_w)\n    train_size = int(0.8 * total)\n    val_size = int(0.1 * total)\n    \n    X_w_train = torch.FloatTensor(X_w[:train_size]).to(device)\n    X_h_train = torch.FloatTensor(X_h[:train_size]).to(device)\n    y_train = torch.FloatTensor(y[:train_size]).unsqueeze(1).to(device)\n    \n    X_w_val = torch.FloatTensor(X_w[train_size:train_size+val_size]).to(device)\n    X_h_val = torch.FloatTensor(X_h[train_size:train_size+val_size]).to(device)\n    y_val = torch.FloatTensor(y[train_size:train_size+val_size]).unsqueeze(1).to(device)\n    \n    # å‰µå»º DataLoader\n    train_dataset = TensorDataset(X_w_train, X_h_train, y_train)\n    val_dataset = TensorDataset(X_w_val, X_h_val, y_val)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    client_dataloaders[client_name] = {\n        'train': train_loader,\n        'val': val_loader,\n        'train_size': len(train_dataset)\n    }\n    \n    print(f\"  âœ“ {client_name}: {len(train_dataset)} è¨“ç·´æ¨£æœ¬, {len(val_dataset)} é©—è­‰æ¨£æœ¬\")\n\nprint(f\"\\nç¸½å…±è¼‰å…¥ {len(client_dataloaders)} å€‹å®¢æˆ¶ç«¯\")\n\n# ============================================================================\n# æ­¥é©Ÿ 2: åˆå§‹åŒ–æ¨¡å‹\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 2ã€‘åˆå§‹åŒ–æ¨¡å‹\")\n\n# === é›²ç«¯ Weather Modelï¼ˆå…¨å±€ï¼Œå¯è¨“ç·´ï¼‰===\nglobal_weather_model = TransformerModel(\n    feature_dim=len(weather_features),\n    d_model=256,\n    nhead=8,\n    num_layers=4,\n    output_dim=None,\n    max_seq_length=seq_length,\n    dropout=0.1\n).to(device)\n\nprint(f\"  âœ“ å…¨å±€ Weather Model å·²åˆå§‹åŒ–ï¼ˆé›²ç«¯ï¼‰\")\nprint(f\"    - åƒæ•¸: {sum(p.numel() for p in global_weather_model.parameters()):,}\")\n\n# === ç‚ºæ¯å€‹å®¢æˆ¶ç«¯å‰µå»ºæœ¬åœ°æ¨¡å‹ ===\nclient_models = {}\n\nfor client_name in client_names:\n    # HFL Modelï¼ˆæœ¬åœ°ï¼Œå‡çµï¼‰\n    hfl_model = TransformerModel(\n        feature_dim=len(hfl_features),\n        d_model=256,\n        nhead=8,\n        num_layers=4,\n        output_dim=None,\n        max_seq_length=seq_length,\n        dropout=0.1\n    ).to(device)\n    \n    # å˜—è©¦è¼‰å…¥å€‹æ€§åŒ–æ¬Šé‡\n    if 'client_models' in locals() and client_name.replace('Consumer_', '') in str(client_models):\n        # è¼‰å…¥å€‹æ€§åŒ–æ¬Šé‡\n        pass  # å¯ä»¥å¾å‰é¢çš„ Personalizer ç²å¾—\n    else:\n        # ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–æˆ–è¼‰å…¥å…¨å±€ HFL æ¨¡å‹\n        pass\n    \n    # å‡çµ HFL Model\n    for param in hfl_model.parameters():\n        param.requires_grad = False\n    hfl_model.eval()\n    \n    # Fusion Modelï¼ˆæœ¬åœ°ï¼Œå¯è¨“ç·´ï¼‰\n    fusion_model = FusionModel(\n        embedding_dim_party_a=256,\n        embedding_dim_party_b=256,\n        hidden_dim=256,\n        output_dim=1,\n        dropout=0.1\n    ).to(device)\n    \n    client_models[client_name] = {\n        'hfl_model': hfl_model,\n        'fusion_model': fusion_model\n    }\n\nprint(f\"  âœ“ ç‚º {len(client_models)} å€‹å®¢æˆ¶ç«¯å‰µå»ºæœ¬åœ°æ¨¡å‹\")\n\n# ============================================================================\n# æ­¥é©Ÿ 3: è¨“ç·´é…ç½®\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 3ã€‘è¨“ç·´é…ç½®\")\n\n# è¨“ç·´åƒæ•¸\nnum_clients = 5  # æ¯è¼ªé¸æ“‡çš„å®¢æˆ¶ç«¯æ•¸é‡\ntotal_rounds = 100  # ç¸½è¨“ç·´è¼ªæ•¸\nphase1_rounds = 10  # å‰ 10 è¼ªï¼šæ¯å€‹ epoch éƒ½è¨“ç·´å…©å€‹æ¨¡å‹\nphase2_rounds = 90  # å¾Œ 90 è¼ªï¼š4 å€‹ epoch åªè¨“ç·´ Fusionï¼Œ1 å€‹ epoch è¨“ç·´é›²ç«¯\n\nlearning_rate = 0.001\ncriterion = nn.MSELoss()\n\nprint(f\"  ã€è¯é‚¦å­¸ç¿’é…ç½®ã€‘\")\nprint(f\"    - ç¸½è¨“ç·´è¼ªæ•¸: {total_rounds}\")\nprint(f\"    - æ¯è¼ªé¸æ“‡å®¢æˆ¶ç«¯æ•¸: {num_clients} / {len(client_names)}\")\nprint(f\"    - éšæ®µ 1 (è¼ª 1-{phase1_rounds}): æ¯å€‹ epoch éƒ½è¨“ç·´ Fusion + Weather\")\nprint(f\"    - éšæ®µ 2 (è¼ª {phase1_rounds+1}-{total_rounds}): 4 epoch è¨“ç·´ Fusionï¼Œ1 epoch è¨“ç·´ Weather\")\nprint(f\"    - å­¸ç¿’ç‡: {learning_rate}\")\n\n# ============================================================================\n# æ­¥é©Ÿ 4: FedAvg æ¢¯åº¦èšåˆå‡½æ•¸\n# ============================================================================\n\ndef aggregate_gradients(client_gradients, client_weights):\n    \"\"\"\n    FedAvg æ¢¯åº¦èšåˆ\n    \n    Args:\n        client_gradients: List of gradients from each client\n        client_weights: List of weights (based on dataset size)\n    \n    Returns:\n        Aggregated gradients\n    \"\"\"\n    aggregated_grads = []\n    \n    # æ­£è¦åŒ–æ¬Šé‡\n    total_weight = sum(client_weights)\n    normalized_weights = [w / total_weight for w in client_weights]\n    \n    # å°æ¯å€‹åƒæ•¸é€²è¡ŒåŠ æ¬Šå¹³å‡\n    for param_idx in range(len(client_gradients[0])):\n        weighted_grad = None\n        for client_idx, grads in enumerate(client_gradients):\n            if weighted_grad is None:\n                weighted_grad = grads[param_idx] * normalized_weights[client_idx]\n            else:\n                weighted_grad += grads[param_idx] * normalized_weights[client_idx]\n        aggregated_grads.append(weighted_grad)\n    \n    return aggregated_grads\n\ndef apply_aggregated_gradients(model, aggregated_grads):\n    \"\"\"å°‡èšåˆå¾Œçš„æ¢¯åº¦æ‡‰ç”¨åˆ°æ¨¡å‹\"\"\"\n    for param, grad in zip(model.parameters(), aggregated_grads):\n        if param.grad is None:\n            param.grad = grad.clone()\n        else:\n            param.grad.copy_(grad)\n\n# ============================================================================\n# æ­¥é©Ÿ 5: è¨“ç·´å¾ªç’°\n# ============================================================================\nprint(f\"\\n{'=' * 80}\")\nprint(\"é–‹å§‹è¯é‚¦å­¸ç¿’è¨“ç·´...\")\nprint(\"=\" * 80)\n\n# è¨˜éŒ„è¨“ç·´æ­·å²\nhistory = {\n    'train_loss': [],\n    'val_loss': [],\n    'selected_clients': []\n}\n\n# å…¨å±€å„ªåŒ–å™¨\nglobal_weather_optimizer = torch.optim.Adam(\n    global_weather_model.parameters(),\n    lr=learning_rate,\n    weight_decay=1e-4\n)\n\nfor round_idx in range(total_rounds):\n    print(f\"\\n{'â”€' * 80}\")\n    print(f\"è¯é‚¦å­¸ç¿’è¼ªæ¬¡ [{round_idx + 1}/{total_rounds}]\")\n    print(f\"{'â”€' * 80}\")\n    \n    # ç¢ºå®šè¨“ç·´ç­–ç•¥\n    if round_idx < phase1_rounds:\n        # éšæ®µ 1: æ¯å€‹ epoch éƒ½è¨“ç·´å…©å€‹æ¨¡å‹\n        train_weather = True\n        phase_name = \"éšæ®µ 1\"\n    else:\n        # éšæ®µ 2: 4 epoch è¨“ç·´ Fusionï¼Œ1 epoch è¨“ç·´ Weather\n        epoch_in_phase2 = (round_idx - phase1_rounds) % 5\n        train_weather = (epoch_in_phase2 == 4)  # æ¯ 5 å€‹ epoch çš„ç¬¬ 5 å€‹è¨“ç·´ Weather\n        phase_name = \"éšæ®µ 2\"\n    \n    print(f\"  ã€{phase_name}ã€‘è¨“ç·´æ¨¡å¼: \", end=\"\")\n    if train_weather:\n        print(\"Fusion Model + Weather Model âš¡\")\n    else:\n        print(\"Fusion Model only (ç¯€çœé€šè¨Š) ğŸ“¡\")\n    \n    # ========================================================================\n    # å®¢æˆ¶ç«¯é¸æ“‡\n    # ========================================================================\n    available_clients = list(client_names)\n    selected_clients = random.sample(available_clients, min(num_clients, len(available_clients)))\n    \n    print(f\"\\n  ã€å®¢æˆ¶ç«¯é¸æ“‡ã€‘\")\n    print(f\"    é¸ä¸­: {selected_clients}\")\n    \n    history['selected_clients'].append(selected_clients)\n    \n    # ========================================================================\n    # å®¢æˆ¶ç«¯æœ¬åœ°è¨“ç·´\n    # ========================================================================\n    client_losses = []\n    client_weather_gradients = []\n    client_sample_counts = []\n    \n    for client_name in selected_clients:\n        # ç²å–å®¢æˆ¶ç«¯æ¨¡å‹å’Œæ•¸æ“š\n        hfl_model = client_models[client_name]['hfl_model']\n        fusion_model = client_models[client_name]['fusion_model']\n        train_loader = client_dataloaders[client_name]['train']\n        \n        # å‰µå»ºæœ¬åœ° Weather Model å‰¯æœ¬ï¼ˆç”¨æ–¼è¨ˆç®—æ¢¯åº¦ï¼‰\n        local_weather_model = copy.deepcopy(global_weather_model)\n        local_weather_model.train()\n        \n        # æœ¬åœ°å„ªåŒ–å™¨\n        fusion_optimizer = torch.optim.Adam(\n            fusion_model.parameters(),\n            lr=learning_rate,\n            weight_decay=1e-4\n        )\n        \n        if train_weather:\n            weather_optimizer = torch.optim.Adam(\n                local_weather_model.parameters(),\n                lr=learning_rate,\n                weight_decay=1e-4\n            )\n        \n        # æœ¬åœ°è¨“ç·´\n        fusion_model.train()\n        epoch_loss = 0.0\n        num_batches = 0\n        \n        for weather_batch, hfl_batch, targets in train_loader:\n            # å‰å‘å‚³æ’­\n            if train_weather:\n                # è¨“ç·´ Weather Model\n                weather_embedding = local_weather_model.forward_embedding(weather_batch)\n            else:\n                # ä¸è¨“ç·´ Weather Modelï¼Œä½¿ç”¨å…¨å±€æ¨¡å‹\n                with torch.no_grad():\n                    weather_embedding = global_weather_model.forward_embedding(weather_batch)\n            \n            with torch.no_grad():\n                hfl_embedding = hfl_model.forward_embedding(hfl_batch)\n            \n            # Fusion é æ¸¬\n            predictions = fusion_model(weather_embedding, hfl_embedding)\n            loss = criterion(predictions, targets)\n            \n            # åå‘å‚³æ’­\n            fusion_optimizer.zero_grad()\n            if train_weather:\n                weather_optimizer.zero_grad()\n            \n            loss.backward()\n            \n            # æ›´æ–° Fusion Model\n            fusion_optimizer.step()\n            \n            # æ›´æ–° Weather Model\n            if train_weather:\n                weather_optimizer.step()\n            \n            epoch_loss += loss.item()\n            num_batches += 1\n        \n        avg_loss = epoch_loss / num_batches\n        client_losses.append(avg_loss)\n        \n        # æ”¶é›† Weather Model æ¢¯åº¦ï¼ˆç”¨æ–¼ FedAvgï¼‰\n        if train_weather:\n            grads = [param.grad.clone() if param.grad is not None else torch.zeros_like(param) \n                     for param in local_weather_model.parameters()]\n            client_weather_gradients.append(grads)\n            client_sample_counts.append(client_dataloaders[client_name]['train_size'])\n        \n        print(f\"    âœ“ {client_name}: Loss = {avg_loss:.6f}\")\n    \n    # ========================================================================\n    # é›²ç«¯èšåˆï¼ˆFedAvgï¼‰\n    # ========================================================================\n    if train_weather and len(client_weather_gradients) > 0:\n        print(f\"\\n  ã€é›²ç«¯èšåˆã€‘FedAvg æ¢¯åº¦èåˆ\")\n        \n        # èšåˆæ¢¯åº¦\n        aggregated_grads = aggregate_gradients(client_weather_gradients, client_sample_counts)\n        \n        # æ‡‰ç”¨èšåˆæ¢¯åº¦åˆ°å…¨å±€æ¨¡å‹\n        apply_aggregated_gradients(global_weather_model, aggregated_grads)\n        global_weather_optimizer.step()\n        \n        print(f\"    âœ“ å…¨å±€ Weather Model å·²æ›´æ–°\")\n        print(f\"    - åƒèˆ‡å®¢æˆ¶ç«¯: {len(client_weather_gradients)}\")\n        print(f\"    - æ¨£æœ¬æ¬Šé‡: {[f'{w/sum(client_sample_counts):.2%}' for w in client_sample_counts]}\")\n    \n    # ========================================================================\n    # å…¨å±€é©—è­‰\n    # ========================================================================\n    global_weather_model.eval()\n    val_losses = []\n    \n    for client_name in selected_clients:\n        fusion_model = client_models[client_name]['fusion_model']\n        hfl_model = client_models[client_name]['hfl_model']\n        val_loader = client_dataloaders[client_name]['val']\n        \n        fusion_model.eval()\n        val_loss = 0.0\n        num_batches = 0\n        \n        with torch.no_grad():\n            for weather_batch, hfl_batch, targets in val_loader:\n                weather_embedding = global_weather_model.forward_embedding(weather_batch)\n                hfl_embedding = hfl_model.forward_embedding(hfl_batch)\n                predictions = fusion_model(weather_embedding, hfl_embedding)\n                loss = criterion(predictions, targets)\n                val_loss += loss.item()\n                num_batches += 1\n        \n        val_losses.append(val_loss / num_batches if num_batches > 0 else 0)\n    \n    # è¨˜éŒ„æ­·å²\n    avg_train_loss = sum(client_losses) / len(client_losses)\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    history['train_loss'].append(avg_train_loss)\n    history['val_loss'].append(avg_val_loss)\n    \n    print(f\"\\n  ã€æœ¬è¼ªçµæœã€‘\")\n    print(f\"    å¹³å‡è¨“ç·´æå¤±: {avg_train_loss:.6f}\")\n    print(f\"    å¹³å‡é©—è­‰æå¤±: {avg_val_loss:.6f}\")\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"âœ“ è¯é‚¦å­¸ç¿’è¨“ç·´å®Œæˆï¼\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# æ­¥é©Ÿ 6: è©•ä¼°èˆ‡å¯è¦–åŒ–\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 6ã€‘å…¨å±€è©•ä¼°\")\n\n# åœ¨æ‰€æœ‰å®¢æˆ¶ç«¯ä¸Šè©•ä¼°\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nall_predictions = []\nall_targets = []\n\nglobal_weather_model.eval()\n\nfor client_name in client_names[:5]:  # è©•ä¼°å‰ 5 å€‹å®¢æˆ¶ç«¯\n    fusion_model = client_models[client_name]['fusion_model']\n    hfl_model = client_models[client_name]['hfl_model']\n    val_loader = client_dataloaders[client_name]['val']\n    \n    fusion_model.eval()\n    \n    with torch.no_grad():\n        for weather_batch, hfl_batch, targets in val_loader:\n            weather_embedding = global_weather_model.forward_embedding(weather_batch)\n            hfl_embedding = hfl_model.forward_embedding(hfl_batch)\n            predictions = fusion_model(weather_embedding, hfl_embedding)\n            \n            all_predictions.extend(predictions.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\nall_predictions = np.array(all_predictions)\nall_targets = np.array(all_targets)\n\nmse = mean_squared_error(all_targets, all_predictions)\nmae = mean_absolute_error(all_targets, all_predictions)\nrmse = np.sqrt(mse)\nr2 = r2_score(all_targets, all_predictions)\n\nprint(f\"\\nå…¨å±€æ€§èƒ½ï¼ˆå‰ 5 å€‹å®¢æˆ¶ç«¯ï¼‰:\")\nprint(f\"  - MSE: {mse:.6f}\")\nprint(f\"  - MAE: {mae:.6f}\")\nprint(f\"  - RMSE: {rmse:.6f}\")\nprint(f\"  - RÂ²: {r2:.6f}\")\n\n# ============================================================================\n# å¯è¦–åŒ–\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 7ã€‘å¯è¦–åŒ–çµæœ\")\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. è¨“ç·´æå¤±æ›²ç·š\naxes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\naxes[0, 0].plot(history['val_loss'], label='Val Loss', color='orange', linewidth=2)\naxes[0, 0].axvline(x=phase1_rounds, color='red', linestyle='--', label='Phase 1â†’2', linewidth=2)\naxes[0, 0].set_xlabel('Round')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Federated Learning: Training Progress')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. é æ¸¬ vs å¯¦éš›\naxes[0, 1].scatter(all_targets, all_predictions, alpha=0.5, s=10)\naxes[0, 1].plot([all_targets.min(), all_targets.max()], \n                [all_targets.min(), all_targets.max()], \n                'r--', lw=2, label='Perfect Prediction')\naxes[0, 1].set_xlabel('Actual')\naxes[0, 1].set_ylabel('Predicted')\naxes[0, 1].set_title(f'Predictions vs Actual (RÂ²={r2:.4f})')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. å®¢æˆ¶ç«¯åƒèˆ‡åˆ†ä½ˆ\nfrom collections import Counter\nclient_participation = Counter([c for round_clients in history['selected_clients'] for c in round_clients])\naxes[1, 0].bar(range(len(client_participation)), list(client_participation.values()), color='skyblue')\naxes[1, 0].set_xlabel('Client Index')\naxes[1, 0].set_ylabel('Participation Count')\naxes[1, 0].set_title('Client Participation Distribution')\naxes[1, 0].grid(True, alpha=0.3, axis='y')\n\n# 4. èª¤å·®åˆ†ä½ˆ\nerrors = all_predictions.flatten() - all_targets.flatten()\naxes[1, 1].hist(errors, bins=50, alpha=0.7, color='green', edgecolor='black')\naxes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\naxes[1, 1].set_xlabel('Prediction Error')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title(f'Error Distribution (MAE={mae:.4f})')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('federated_vfl_results.png', dpi=150, bbox_inches='tight')\nprint(f\"  âœ“ å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜: federated_vfl_results.png\")\nplt.show()\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"VFL è¯é‚¦å­¸ç¿’å®Œæˆï¼\")\nprint(\"=\" * 80)\nprint(f\"\\næœ€çµ‚çµ±è¨ˆ:\")\nprint(f\"  - ç¸½è¨“ç·´è¼ªæ•¸: {total_rounds}\")\nprint(f\"  - åƒèˆ‡å®¢æˆ¶ç«¯ç¸½æ•¸: {len(client_names)}\")\nprint(f\"  - æ¯è¼ªé¸æ“‡: {num_clients} å€‹å®¢æˆ¶ç«¯\")\nprint(f\"  - Weather Model æ›´æ–°æ¬¡æ•¸: {phase1_rounds + phase2_rounds // 5}\")\nprint(f\"  - é€šè¨Šç¯€çœ: {(1 - (phase1_rounds + phase2_rounds // 5) / total_rounds) * 100:.1f}%\")\nprint(f\"\\næ¨¡å‹æ¶æ§‹:\")\nprint(f\"  âœ“ Weather Model: é›²ç«¯ï¼ˆå…±äº«ï¼‰\")\nprint(f\"  âœ“ HFL Models: æœ¬åœ°ï¼ˆæ¯å€‹å®¢æˆ¶ç«¯ï¼Œå‡çµï¼‰\")\nprint(f\"  âœ“ Fusion Models: æœ¬åœ°ï¼ˆæ¯å€‹å®¢æˆ¶ç«¯ï¼Œå¯è¨“ç·´ï¼‰\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jb19x9yc2hs",
   "source": "# ============================================================================\n# åæ¨™æº–åŒ–èˆ‡è©³ç´°å¯è¦–åŒ–\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"åæ¨™æº–åŒ–èˆ‡è©³ç´°å¯è¦–åŒ–\")\nprint(\"=\" * 80)\n\n# ============================================================================\n# æ­¥é©Ÿ 1: åæ¨™æº–åŒ–é æ¸¬çµæœ\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 1ã€‘åæ¨™æº–åŒ–é æ¸¬çµæœ\")\n\n# åæ¨™æº–åŒ–é æ¸¬å€¼å’Œå¯¦éš›å€¼\npredictions_original = target_scaler.inverse_transform(all_predictions.reshape(-1, 1)).flatten()\ntargets_original = target_scaler.inverse_transform(all_targets.reshape(-1, 1)).flatten()\n\nprint(f\"  âœ“ å·²å®Œæˆåæ¨™æº–åŒ–\")\nprint(f\"    - é æ¸¬å€¼ç¯„åœ: [{predictions_original.min():.2f}, {predictions_original.max():.2f}]\")\nprint(f\"    - å¯¦éš›å€¼ç¯„åœ: [{targets_original.min():.2f}, {targets_original.max():.2f}]\")\n\n# é‡æ–°è¨ˆç®—è©•ä¼°æŒ‡æ¨™ï¼ˆä½¿ç”¨åŸå§‹å°ºåº¦ï¼‰\nmse_original = mean_squared_error(targets_original, predictions_original)\nmae_original = mean_absolute_error(targets_original, predictions_original)\nrmse_original = np.sqrt(mse_original)\nr2_original = r2_score(targets_original, predictions_original)\n\n# è¨ˆç®— sMAPE (Symmetric Mean Absolute Percentage Error)\ndef smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0  # é¿å…é™¤é›¶\n    return np.mean(diff) * 100\n\nsmape_value = smape(targets_original, predictions_original)\n\nprint(f\"\\nã€åŸå§‹å°ºåº¦è©•ä¼°æŒ‡æ¨™ã€‘\")\nprint(f\"  - MSE: {mse_original:.4f}\")\nprint(f\"  - MAE: {mae_original:.4f}\")\nprint(f\"  - RMSE: {rmse_original:.4f}\")\nprint(f\"  - RÂ²: {r2_original:.4f}\")\nprint(f\"  - sMAPE: {smape_value:.2f}%\")\n\n# ============================================================================\n# æ­¥é©Ÿ 2: è©³ç´°å¯è¦–åŒ–\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 2ã€‘ç”Ÿæˆè©³ç´°å¯è¦–åŒ–\")\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# === åœ– 1: è¨“ç·´æå¤±æ›²ç·š ===\nax1 = fig.add_subplot(gs[0, 0])\nax1.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\nax1.plot(history['val_loss'], label='Val Loss', color='orange', linewidth=2)\nax1.axvline(x=phase1_rounds, color='red', linestyle='--', label='Phase 1â†’2', linewidth=2)\nax1.set_xlabel('Round', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Federated Learning: Training Progress', fontsize=14, fontweight='bold')\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3)\n\n# === åœ– 2: é æ¸¬ vs å¯¦éš›ï¼ˆæ•£é»åœ–ï¼ŒåŸå§‹å°ºåº¦ï¼‰===\nax2 = fig.add_subplot(gs[0, 1])\nax2.scatter(targets_original, predictions_original, alpha=0.5, s=20, c='steelblue', edgecolors='black', linewidth=0.5)\nax2.plot([targets_original.min(), targets_original.max()], \n         [targets_original.min(), targets_original.max()], \n         'r--', lw=3, label='Perfect Prediction')\nax2.set_xlabel('Actual Power Demand (åŸå§‹å€¼)', fontsize=12)\nax2.set_ylabel('Predicted Power Demand (åŸå§‹å€¼)', fontsize=12)\nax2.set_title(f'Predictions vs Actual (RÂ²={r2_original:.4f})', fontsize=14, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\n# === åœ– 3: å®¢æˆ¶ç«¯åƒèˆ‡åˆ†ä½ˆ ===\nax3 = fig.add_subplot(gs[0, 2])\nfrom collections import Counter\nclient_participation = Counter([c for round_clients in history['selected_clients'] for c in round_clients])\nclient_names_short = [name.replace('Consumer_', 'C') for name in client_participation.keys()]\nax3.bar(range(len(client_participation)), list(client_participation.values()), \n        color='skyblue', edgecolor='black', linewidth=1.5)\nax3.set_xlabel('Client', fontsize=12)\nax3.set_ylabel('Participation Count', fontsize=12)\nax3.set_title('Client Participation Distribution', fontsize=14, fontweight='bold')\nax3.set_xticks(range(len(client_participation)))\nax3.set_xticklabels(client_names_short, rotation=45)\nax3.grid(True, alpha=0.3, axis='y')\n\n# === åœ– 4: é æ¸¬èˆ‡å¯¦éš›æŠ˜ç·šåœ–ï¼ˆå®Œæ•´åºåˆ—ï¼ŒåŸå§‹å°ºåº¦ï¼‰===\nax4 = fig.add_subplot(gs[1, :])\nsample_size = min(500, len(targets_original))  # é¡¯ç¤ºå‰ 500 å€‹æ¨£æœ¬\ntime_steps = np.arange(sample_size)\nax4.plot(time_steps, targets_original[:sample_size], \n         label='Actual Power Demand', color='blue', linewidth=2, alpha=0.8)\nax4.plot(time_steps, predictions_original[:sample_size], \n         label='Predicted Power Demand', color='red', linewidth=1.5, alpha=0.7, linestyle='--')\nax4.fill_between(time_steps, \n                  targets_original[:sample_size], \n                  predictions_original[:sample_size], \n                  alpha=0.2, color='gray', label='Prediction Error')\nax4.set_xlabel('Time Step', fontsize=12)\nax4.set_ylabel('Power Demand (åŸå§‹å€¼)', fontsize=12)\nax4.set_title(f'Time Series: Prediction vs Actual (å‰ {sample_size} å€‹æ¨£æœ¬)', fontsize=14, fontweight='bold')\nax4.legend(fontsize=10, loc='upper right')\nax4.grid(True, alpha=0.3)\n\n# === åœ– 5: é æ¸¬èˆ‡å¯¦éš›æŠ˜ç·šåœ–ï¼ˆå±€éƒ¨æ”¾å¤§ï¼‰===\nax5 = fig.add_subplot(gs[2, 0])\nzoom_size = min(100, len(targets_original))\ntime_steps_zoom = np.arange(zoom_size)\nax5.plot(time_steps_zoom, targets_original[:zoom_size], \n         label='Actual', color='blue', linewidth=3, marker='o', markersize=4)\nax5.plot(time_steps_zoom, predictions_original[:zoom_size], \n         label='Predicted', color='red', linewidth=2, marker='s', markersize=3, linestyle='--')\nax5.set_xlabel('Time Step', fontsize=12)\nax5.set_ylabel('Power Demand', fontsize=12)\nax5.set_title(f'å±€éƒ¨æ”¾å¤§ (å‰ {zoom_size} å€‹æ¨£æœ¬)', fontsize=14, fontweight='bold')\nax5.legend(fontsize=10)\nax5.grid(True, alpha=0.3)\n\n# === åœ– 6: èª¤å·®åˆ†ä½ˆï¼ˆåŸå§‹å°ºåº¦ï¼‰===\nax6 = fig.add_subplot(gs[2, 1])\nerrors_original = predictions_original - targets_original\nax6.hist(errors_original, bins=50, alpha=0.7, color='green', edgecolor='black', linewidth=1.5)\nax6.axvline(x=0, color='red', linestyle='--', linewidth=3, label='Zero Error')\nax6.axvline(x=np.mean(errors_original), color='orange', linestyle='--', linewidth=2, label=f'Mean Error: {np.mean(errors_original):.2f}')\nax6.set_xlabel('Prediction Error (åŸå§‹å€¼)', fontsize=12)\nax6.set_ylabel('Frequency', fontsize=12)\nax6.set_title(f'Error Distribution (MAE={mae_original:.2f})', fontsize=14, fontweight='bold')\nax6.legend(fontsize=10)\nax6.grid(True, alpha=0.3, axis='y')\n\n# === åœ– 7: èª¤å·®ç™¾åˆ†æ¯”åˆ†ä½ˆ ===\nax7 = fig.add_subplot(gs[2, 2])\npercentage_errors = (errors_original / targets_original) * 100\npercentage_errors = percentage_errors[~np.isinf(percentage_errors)]  # ç§»é™¤ç„¡çª®å¤§\nax7.hist(percentage_errors, bins=50, alpha=0.7, color='purple', edgecolor='black', linewidth=1.5)\nax7.axvline(x=0, color='red', linestyle='--', linewidth=3, label='Zero Error')\nax7.set_xlabel('Prediction Error (%)', fontsize=12)\nax7.set_ylabel('Frequency', fontsize=12)\nax7.set_title(f'Percentage Error Distribution (sMAPE={smape_value:.2f}%)', fontsize=14, fontweight='bold')\nax7.legend(fontsize=10)\nax7.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('federated_vfl_detailed_results.png', dpi=150, bbox_inches='tight')\nprint(f\"  âœ“ è©³ç´°å¯è¦–åŒ–åœ–è¡¨å·²ä¿å­˜: federated_vfl_detailed_results.png\")\nplt.show()\n\n# ============================================================================\n# æ­¥é©Ÿ 3: æ€§èƒ½çµ±è¨ˆæ‘˜è¦\n# ============================================================================\nprint(f\"\\nã€æ­¥é©Ÿ 3ã€‘æ€§èƒ½çµ±è¨ˆæ‘˜è¦\")\n\nprint(f\"\\næ¨™æº–åŒ–å°ºåº¦ï¼ˆè¨“ç·´æ™‚ï¼‰:\")\nprint(f\"  - MSE: {mse:.6f}\")\nprint(f\"  - MAE: {mae:.6f}\")\nprint(f\"  - RMSE: {rmse:.6f}\")\nprint(f\"  - RÂ²: {r2:.6f}\")\n\nprint(f\"\\nåŸå§‹å°ºåº¦ï¼ˆå¯¦éš›æ‡‰ç”¨ï¼‰:\")\nprint(f\"  - MSE: {mse_original:.4f}\")\nprint(f\"  - MAE: {mae_original:.4f} (å¹³å‡çµ•å°èª¤å·®)\")\nprint(f\"  - RMSE: {rmse_original:.4f} (å‡æ–¹æ ¹èª¤å·®)\")\nprint(f\"  - RÂ²: {r2_original:.4f} (æ±ºå®šä¿‚æ•¸)\")\nprint(f\"  - sMAPE: {smape_value:.2f}% (å°ç¨±å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®)\")\n\nprint(f\"\\nèª¤å·®çµ±è¨ˆ:\")\nprint(f\"  - å¹³å‡èª¤å·®: {np.mean(errors_original):.4f}\")\nprint(f\"  - ä¸­ä½æ•¸èª¤å·®: {np.median(errors_original):.4f}\")\nprint(f\"  - èª¤å·®æ¨™æº–å·®: {np.std(errors_original):.4f}\")\nprint(f\"  - æœ€å¤§æ­£èª¤å·®: {np.max(errors_original):.4f}\")\nprint(f\"  - æœ€å¤§è² èª¤å·®: {np.min(errors_original):.4f}\")\n\nprint(f\"\\nè¯é‚¦å­¸ç¿’çµ±è¨ˆ:\")\nprint(f\"  - ç¸½è¨“ç·´è¼ªæ•¸: {total_rounds}\")\nprint(f\"  - åƒèˆ‡å®¢æˆ¶ç«¯ç¸½æ•¸: {len(client_names)}\")\nprint(f\"  - æ¯è¼ªé¸æ“‡: {num_clients} å€‹å®¢æˆ¶ç«¯\")\nprint(f\"  - Weather Model æ›´æ–°æ¬¡æ•¸: {phase1_rounds + phase2_rounds // 5}\")\nprint(f\"  - é€šè¨Šç¯€çœæ¯”ä¾‹: {(1 - (phase1_rounds + phase2_rounds // 5) / total_rounds) * 100:.1f}%\")\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"VFL è¯é‚¦å­¸ç¿’èˆ‡è©³ç´°åˆ†æå®Œæˆï¼\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}