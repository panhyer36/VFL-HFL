"""
VFL è¨“ç·´ä¸»è…³æœ¬ - å‚ç›´è¯é‚¦å­¸ç¿’ + FedAvg

**è¨“ç·´æµç¨‹**:
1. åˆå§‹åŒ– Server (å…¨å±€ Weather Model)
2. åˆå§‹åŒ– Clients (HFL Model + Fusion Model)
3. è¼‰å…¥ä¸¦é è™•ç†æ•¸æ“š (Weather + HFL)
4. è¯é‚¦å­¸ç¿’è¨“ç·´å¾ªç’° (FedAvg)
5. ä¿å­˜æ¨¡å‹å’Œå¯è¦–åŒ–çµæœ

**æ•¸æ“šæµ**:
- Server: ç®¡ç† Weather Modelï¼Œèšåˆæ¢¯åº¦
- Clients:
  * Weather æ•¸æ“š â†’ Weather Model â†’ Weather åµŒå…¥
  * HFL æ•¸æ“š â†’ HFL Model â†’ HFL åµŒå…¥
  * é›™æ–¹åµŒå…¥ â†’ Fusion Model â†’ é æ¸¬

**é€šè¨Šå„ªåŒ–**:
- éšæ®µ1: æ¯è¼ªéƒ½è¨“ç·´ Fusion + Weather
- éšæ®µ2: 4è¼ªè¨“ç·´ Fusionï¼Œ1è¼ªè¨“ç·´ Weather (ç¯€çœé€šè¨Š)
"""

import argparse
import os
import glob
import torch
import numpy as np
import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from config import load_config
from Server import VFLServer
from Client import VFLClient
from Personalizer import initialize_personalized_models


def load_weather_data(config):
    """
    è¼‰å…¥ Weather æ•¸æ“šä¸¦æ¨™æº–åŒ–

    Args:
        config: é…ç½®å°è±¡

    Returns:
        weather_data_scaled: æ¨™æº–åŒ–å¾Œçš„ Weather æ•¸æ“š
        weather_scaler: Weather æ¨™æº–åŒ–å™¨
    """
    print(f"\n{'=' * 70}")
    print("è¼‰å…¥ Weather æ•¸æ“š (é›²ç«¯)")
    print(f"{'=' * 70}")

    # è®€å– Weather CSV
    weather_csv_path = os.path.join(config.data_path, f"{config.weather_csv}.csv")
    weather_df = pd.read_csv(weather_csv_path)

    print(f"  - Weather åŸå§‹æ•¸æ“šå½¢ç‹€: {weather_df.shape}")
    print(f"  - Weather ç‰¹å¾µæ•¸: {len(config.weather_features)}")

    # æå– Weather ç‰¹å¾µ
    weather_data_raw = weather_df[config.weather_features].values

    # æ¨™æº–åŒ–å™¨è·¯å¾‘
    weather_scaler_path = os.path.join(config.data_path, "weather_scaler.pkl")

    if os.path.exists(weather_scaler_path):
        # è¼‰å…¥å·²æœ‰çš„æ¨™æº–åŒ–å™¨
        with open(weather_scaler_path, 'rb') as f:
            weather_scaler = pickle.load(f)
        print(f"  âœ“ Weather æ¨™æº–åŒ–å™¨å·²è¼‰å…¥")
    else:
        # å‰µå»ºæ–°çš„æ¨™æº–åŒ–å™¨
        weather_scaler = StandardScaler()
        weather_scaler.fit(weather_data_raw)
        with open(weather_scaler_path, 'wb') as f:
            pickle.dump(weather_scaler, f)
        print(f"  âœ“ Weather æ¨™æº–åŒ–å™¨å·²å‰µå»ºä¸¦ä¿å­˜")

    # æ¨™æº–åŒ–
    weather_data_scaled = weather_scaler.transform(weather_data_raw)
    print(f"  âœ“ Weather æ•¸æ“šå·²æ¨™æº–åŒ–: {weather_data_scaled.shape}")

    return weather_data_scaled, weather_scaler


def create_weather_sequences(weather_data, seq_len, total_len):
    """
    å‰µå»º Weather æ™‚åºåºåˆ—

    Args:
        weather_data: Weather æ•¸æ“š (æ¨™æº–åŒ–å¾Œ)
        seq_len: åºåˆ—é•·åº¦
        total_len: éœ€è¦çš„åºåˆ—ç¸½æ•¸ (èˆ‡ HFL å°é½Š)

    Returns:
        weather_sequences: Weather åºåˆ—æ•¸çµ„
    """
    sequences = []
    for i in range(min(len(weather_data) - seq_len + 1, total_len)):
        sequences.append(weather_data[i:i + seq_len])
    return np.array(sequences)


def load_client_data(config, weather_sequences, client_csv_files):
    """
    è¼‰å…¥æ‰€æœ‰å®¢æˆ¶ç«¯çš„ HFL æ•¸æ“šä¸¦å‰µå»º DataLoader

    Args:
        config: é…ç½®å°è±¡
        weather_sequences: Weather åºåˆ—æ•¸çµ„
        client_csv_files: å®¢æˆ¶ç«¯ CSV æ–‡ä»¶è·¯å¾‘åˆ—è¡¨

    Returns:
        client_dataloaders: å­—å…¸ {client_name: {'train': loader, 'val': loader, 'train_size': int}}
        client_names: å®¢æˆ¶ç«¯åç¨±åˆ—è¡¨
        target_scaler: ç›®æ¨™è®Šé‡æ¨™æº–åŒ–å™¨
    """
    print(f"\n{'=' * 70}")
    print("è¼‰å…¥å®¢æˆ¶ç«¯æ•¸æ“š (æœ¬åœ°)")
    print(f"{'=' * 70}")

    from DataLoader import SequenceCSVDataset

    client_dataloaders = {}
    client_names = []
    target_scaler = None
    hfl_scaler = None

    # åºåˆ—åƒæ•¸
    seq_length = config.input_length
    output_length = config.output_length
    batch_size = config.batch_size

    for idx, csv_file in enumerate(client_csv_files):
        client_name = os.path.basename(csv_file).replace('.csv', '')
        client_names.append(client_name)

        print(f"\nå®¢æˆ¶ç«¯ [{idx + 1}/{len(client_csv_files)}]: {client_name}")

        # è®€å–å®¢æˆ¶ç«¯æ•¸æ“š
        client_df = pd.read_csv(csv_file)

        # æª¢æŸ¥ç›®æ¨™è®Šé‡
        if config.target[0] not in client_df.columns:
            target_col = 'Consumption_Total'
        else:
            target_col = config.target[0]

        # æå–ç‰¹å¾µå’Œç›®æ¨™
        client_hfl_data = client_df[config.hfl_features].values
        client_target_data = client_df[target_col].values

        # æ¨™æº–åŒ–å™¨ (ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯å‰µå»ºï¼Œå…¶é¤˜å…±ç”¨)
        if idx == 0:
            hfl_scaler = StandardScaler()
            hfl_scaler.fit(client_hfl_data)
            target_scaler = StandardScaler()
            target_scaler.fit(client_target_data.reshape(-1, 1))

            # ä¿å­˜æ¨™æº–åŒ–å™¨
            with open(os.path.join(config.data_path, 'hfl_scaler.pkl'), 'wb') as f:
                pickle.dump(hfl_scaler, f)
            with open(os.path.join(config.data_path, 'target_scaler.pkl'), 'wb') as f:
                pickle.dump(target_scaler, f)

        # æ¨™æº–åŒ–
        client_hfl_scaled = hfl_scaler.transform(client_hfl_data)
        client_target_scaled = target_scaler.transform(client_target_data.reshape(-1, 1)).flatten()

        # å°é½Šé•·åº¦
        min_len = min(len(weather_sequences), len(client_hfl_scaled), len(client_target_scaled))

        # å‰µå»ºåºåˆ—
        def create_sequences(weather, hfl, targets, seq_len):
            X_w, X_h, y = [], [], []
            for i in range(len(weather) - seq_len):
                X_w.append(weather[i])
                X_h.append(hfl[i:i + seq_len])
                y.append(targets[i + seq_len])
            return np.array(X_w), np.array(X_h), np.array(y)

        X_w, X_h, y = create_sequences(
            weather_sequences[:min_len],
            client_hfl_scaled[:min_len],
            client_target_scaled[:min_len],
            seq_length
        )

        # åˆ†å‰²æ•¸æ“šé›† (8:1:1)
        total = len(X_w)
        train_size = int(config.train_ratio * total)
        val_size = int(config.val_ratio * total)

        # è¨“ç·´é›†
        X_w_train = torch.FloatTensor(X_w[:train_size]).to(config.device)
        X_h_train = torch.FloatTensor(X_h[:train_size]).to(config.device)
        y_train = torch.FloatTensor(y[:train_size]).unsqueeze(1).to(config.device)

        # é©—è­‰é›†
        X_w_val = torch.FloatTensor(X_w[train_size:train_size + val_size]).to(config.device)
        X_h_val = torch.FloatTensor(X_h[train_size:train_size + val_size]).to(config.device)
        y_val = torch.FloatTensor(y[train_size:train_size + val_size]).unsqueeze(1).to(config.device)

        # å‰µå»º DataLoader
        train_dataset = TensorDataset(X_w_train, X_h_train, y_train)
        val_dataset = TensorDataset(X_w_val, X_h_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        client_dataloaders[client_name] = {
            'train': train_loader,
            'val': val_loader,
            'train_size': len(train_dataset)
        }

        print(f"  âœ“ è¨“ç·´æ¨£æœ¬: {len(train_dataset)}, é©—è­‰æ¨£æœ¬: {len(val_dataset)}")

    print(f"\nç¸½å…±è¼‰å…¥ {len(client_dataloaders)} å€‹å®¢æˆ¶ç«¯")
    return client_dataloaders, client_names, target_scaler


def train(args):
    """
    VFL è¨“ç·´ä¸»å‡½æ•¸

    Args:
        args: å‘½ä»¤è¡Œåƒæ•¸
    """
    print("\n" + "=" * 70)
    print("VFL å‚ç›´è¯é‚¦å­¸ç¿’è¨“ç·´ - FedAvg")
    print("=" * 70)

    # === æ­¥é©Ÿ 1: è¼‰å…¥é…ç½® ===
    config = load_config(args.config)
    device = config.device

    print(f"\né…ç½®æ‘˜è¦:")
    print(f"  - æ¼”ç®—æ³•: {config.algorithm}")
    print(f"  - å…¨å±€è¼ªæ•¸: {config.K}")
    print(f"  - å®¢æˆ¶ç«¯æ•¸: {config.num_users}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"  - å­¸ç¿’ç‡: {config.beta}")
    print(f"  - è¨­å‚™: {device}")

    # === æ­¥é©Ÿ 2: è¼‰å…¥ Weather æ•¸æ“š ===
    weather_data_scaled, weather_scaler = load_weather_data(config)

    # === æ­¥é©Ÿ 3: è¼‰å…¥å®¢æˆ¶ç«¯æ•¸æ“š ===
    # ç²å–å®¢æˆ¶ç«¯ CSV æ–‡ä»¶
    csv_pattern = os.path.join(config.data_path, config.hfl_csv_pattern + ".csv")
    client_csv_files = sorted(glob.glob(csv_pattern))[:config.num_users]

    if not client_csv_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°å®¢æˆ¶ç«¯æ•¸æ“š: {csv_pattern}")

    print(f"\næ‰¾åˆ° {len(client_csv_files)} å€‹å®¢æˆ¶ç«¯æ–‡ä»¶")

    # å‰µå»º Weather åºåˆ— (èˆ‡ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯å°é½Š)
    # é€™è£¡å…ˆä¼°ç®—åºåˆ—æ•¸é‡
    first_client_df = pd.read_csv(client_csv_files[0])
    total_hfl_sequences = len(first_client_df) - config.input_length
    weather_sequences = create_weather_sequences(
        weather_data_scaled,
        config.input_length,
        total_hfl_sequences
    )

    print(f"\nWeather åºåˆ—å·²å‰µå»º: {weather_sequences.shape}")

    # è¼‰å…¥æ‰€æœ‰å®¢æˆ¶ç«¯æ•¸æ“š
    client_dataloaders, client_names, target_scaler = load_client_data(
        config,
        weather_sequences,
        client_csv_files
    )

    # === æ­¥é©Ÿ 4: åˆå§‹åŒ– Per-FedAvg å€‹æ€§åŒ–æ¨¡å‹ (å¯é¸) ===
    client_hfl_models = {}
    if config.use_personalized_hfl and config.hfl_model_path:
        print(f"\n{'=' * 70}")
        print("Per-FedAvg å€‹æ€§åŒ– HFL æ¨¡å‹åˆå§‹åŒ–")
        print(f"{'=' * 70}")
        try:
            import torch
            from Model import TransformerModel
            from DataLoader import SequenceCSVDataset

            # æª¢æŸ¥ HFL å…¨å±€æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if os.path.exists(config.hfl_model_path):
                print(f"  âœ“ æ‰¾åˆ° HFL å…¨å±€æ¨¡å‹: {config.hfl_model_path}")

                # å‰µå»º HFL æ¨¡å‹æ¶æ§‹
                global_hfl_model = TransformerModel(
                    feature_dim=config.hfl_feature_dim,
                    d_model=config.hfl_d_model,
                    nhead=config.hfl_nhead,
                    num_layers=config.hfl_num_layers,
                    output_dim=config.hfl_output_dim,
                    max_seq_length=config.hfl_max_seq_length,
                    dropout=config.hfl_dropout
                ).to(device)

                # è¼‰å…¥å…¨å±€æ¨¡å‹æ¬Šé‡
                global_hfl_model.load_state_dict(torch.load(config.hfl_model_path, map_location=device))
                print(f"  âœ“ æˆåŠŸè¼‰å…¥ HFL å…¨å±€æ¨¡å‹æ¬Šé‡")

                # ç‚ºæ¯å€‹å®¢æˆ¶ç«¯é€²è¡Œå€‹æ€§åŒ–é©æ‡‰
                print(f"\n  é–‹å§‹ç‚ºæ¯å€‹å®¢æˆ¶ç«¯é€²è¡Œå€‹æ€§åŒ–é©æ‡‰...")
                print(f"  é©æ‡‰åƒæ•¸: lr={config.adaptation_lr}, steps={config.personalization_steps}")
                print()

                for i, csv_file in enumerate(client_csv_files):
                    client_name = os.path.basename(csv_file).replace('.csv', '')
                    print(f"  [{i+1}/{len(client_csv_files)}] {client_name}:")

                    # è¼‰å…¥å®¢æˆ¶ç«¯æ•¸æ“šé›† (ç”¨æ–¼å€‹æ€§åŒ–)
                    try:
                        client_dataset = SequenceCSVDataset(
                            csv_path=os.path.dirname(csv_file),
                            csv_name=client_name,
                            input_len=config.input_length,
                            output_len=config.output_length,
                            features=config.hfl_features,
                            target=config.target,
                            save_path=os.path.dirname(csv_file),
                            train_ratio=config.train_ratio,
                            val_ratio=config.val_ratio,
                            split_type='time_based',
                            fit_scalers=False  # ä½¿ç”¨å·²ä¿å­˜çš„æ¨™æº–åŒ–å™¨
                        )

                        # ä½¿ç”¨ Personalizer é€²è¡Œå€‹æ€§åŒ–é©æ‡‰
                        from Personalizer import personalize_model_for_client
                        personalized_state = personalize_model_for_client(
                            global_model=global_hfl_model,
                            dataset=client_dataset,
                            config=config,
                            client_name=client_name
                        )

                        # ä¿å­˜å€‹æ€§åŒ–å¾Œçš„æ¨¡å‹æ¬Šé‡
                        client_hfl_models[client_name] = personalized_state

                    except Exception as e:
                        print(f"    âš  å€‹æ€§åŒ–å¤±æ•—: {e}")
                        print(f"    â†’ ä½¿ç”¨å…¨å±€æ¨¡å‹æ¬Šé‡")
                        client_hfl_models[client_name] = global_hfl_model.state_dict()

                print(f"\n  âœ“ å·²ç‚º {len(client_hfl_models)} å€‹å®¢æˆ¶ç«¯å®Œæˆå€‹æ€§åŒ–é©æ‡‰")
            else:
                print(f"  âš  HFL å…¨å±€æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config.hfl_model_path}")
                print(f"  â†’ å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„ HFL æ¨¡å‹")
        except Exception as e:
            import traceback
            print(f"  âš  è¼‰å…¥/å€‹æ€§åŒ– HFL æ¨¡å‹å¤±æ•—: {e}")
            print(traceback.format_exc())
            print(f"  â†’ å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–çš„ HFL æ¨¡å‹")

    # === æ­¥é©Ÿ 5: åˆå§‹åŒ– Server å’Œ Clients ===
    print(f"\n{'=' * 70}")
    print("åˆå§‹åŒ– VFL Server å’Œ Clients")
    print(f"{'=' * 70}")

    # åˆå§‹åŒ– Server
    server = VFLServer(config, device)

    # åˆå§‹åŒ– Clients
    clients = {}
    for client_name in client_names:
        hfl_state_dict = client_hfl_models.get(client_name, None)
        client = VFLClient(
            client_id=client_name,
            config=config,
            device=device,
            hfl_model_state_dict=hfl_state_dict
        )
        clients[client_name] = client

    # === æ­¥é©Ÿ 6: è¯é‚¦å­¸ç¿’è¨“ç·´å¾ªç’° ===
    print(f"\n{'=' * 70}")
    print("é–‹å§‹è¯é‚¦å­¸ç¿’è¨“ç·´...")
    print(f"{'=' * 70}")

    for round_idx in range(config.K):
        server.current_round = round_idx

        print(f"\n{'â”€' * 70}")
        print(f"è¯é‚¦å­¸ç¿’è¼ªæ¬¡ [{round_idx + 1}/{config.K}]")
        print(f"{'â”€' * 70}")

        # ç¢ºå®šè¨“ç·´ç­–ç•¥
        train_weather = server.should_update_weather()

        if train_weather:
            print(f"  è¨“ç·´æ¨¡å¼: Fusion Model + Weather Model âš¡")
        else:
            print(f"  è¨“ç·´æ¨¡å¼: Fusion Model only (ç¯€çœé€šè¨Š) ğŸ“¡")

        # å®¢æˆ¶ç«¯é¸æ“‡
        selected_clients = server.select_clients(client_names)
        print(f"\n  é¸ä¸­å®¢æˆ¶ç«¯: {selected_clients}")

        # === Split Learning å‰å‘å‚³æ’­: Server è¨ˆç®— Weather åµŒå…¥ ===
        print(f"\n  Server è¨ˆç®— Weather åµŒå…¥å‘é‡:")

        # æ”¶é›†æ‰€æœ‰é¸ä¸­å®¢æˆ¶ç«¯çš„ Weather æ•¸æ“š
        client_weather_data = {}
        for client_name in selected_clients:
            train_loader = client_dataloaders[client_name]['train']
            val_loader = client_dataloaders[client_name]['val']

            # æå– Weather æ•¸æ“š (è¨“ç·´é›†)
            train_weather_batches = []
            for weather_batch, _, _ in train_loader:
                train_weather_batches.append(weather_batch)
            train_weather_data = torch.cat(train_weather_batches, dim=0).to(device)

            # æå– Weather æ•¸æ“š (é©—è­‰é›†)
            val_weather_batches = []
            for weather_batch, _, _ in val_loader:
                val_weather_batches.append(weather_batch)
            val_weather_data = torch.cat(val_weather_batches, dim=0).to(device)

            client_weather_data[client_name] = {
                'train': train_weather_data,
                'val': val_weather_data
            }

        # Server è¨ˆç®—åµŒå…¥å‘é‡
        client_weather_embeddings = {}
        for client_name in selected_clients:
            # è¨“ç·´é›†åµŒå…¥ (éœ€è¦æ¢¯åº¦)
            train_embeddings = server.compute_weather_embeddings(
                client_weather_data[client_name]['train'],
                requires_grad=train_weather
            )

            # é©—è­‰é›†åµŒå…¥ (ä¸éœ€è¦æ¢¯åº¦)
            val_embeddings = server.compute_weather_embeddings(
                client_weather_data[client_name]['val'],
                requires_grad=False
            )

            client_weather_embeddings[client_name] = {
                'train': train_embeddings,
                'val': val_embeddings
            }

            print(f"    âœ“ {client_name}: Train Embeddings {train_embeddings.shape}, Val Embeddings {val_embeddings.shape}")

        # === å®¢æˆ¶ç«¯æœ¬åœ°è¨“ç·´ (ä½¿ç”¨ Server ç™¼é€çš„åµŒå…¥) ===
        client_losses = []
        client_val_losses = []
        client_embedding_gradients = []
        client_sample_counts = []

        print(f"\n  æœ¬åœ°è¨“ç·´ (Client ç«¯):")
        for client_name in selected_clients:
            client = clients[client_name]
            train_loader = client_dataloaders[client_name]['train']
            val_loader = client_dataloaders[client_name]['val']

            # æœ¬åœ°è¨“ç·´ (æ¥æ”¶ Server çš„åµŒå…¥)
            train_loss, embedding_grad, num_samples = client.local_train(
                train_loader,
                weather_embeddings=client_weather_embeddings[client_name]['train'],
                train_weather=train_weather
            )

            # æœ¬åœ°é©—è­‰
            val_loss = client.local_evaluate(
                val_loader,
                weather_embeddings=client_weather_embeddings[client_name]['val']
            )

            client_losses.append(train_loss)
            client_val_losses.append(val_loss)

            if train_weather and len(embedding_grad) > 0:
                client_embedding_gradients.append(embedding_grad)
                client_sample_counts.append(num_samples)

            print(f"    âœ“ {client_name}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")

        # === Server èšåˆ Embedding æ¢¯åº¦ä¸¦æ›´æ–° Weather Model ===
        if train_weather and client_embedding_gradients:
            print(f"\n  Server èšåˆ Embedding æ¢¯åº¦ä¸¦æ›´æ–° Weather Model (Split Learning + FedAvg):")

            # ä½¿ç”¨ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯çš„ Weather æ•¸æ“šé€²è¡Œåå‘å‚³æ’­ (æ‰€æœ‰å®¢æˆ¶ç«¯å…±äº«ç›¸åŒçš„ Weather æ•¸æ“š)
            representative_client = selected_clients[0]
            weather_data_for_backward = client_weather_data[representative_client]['train']

            server.update_weather_model_from_embeddings(
                weather_data_for_backward,
                client_embedding_gradients,
                client_sample_counts
            )
            print(f"    âœ“ å…¨å±€ Weather Model å·²æ›´æ–° (Chain Rule)")
            print(f"    - åƒèˆ‡å®¢æˆ¶ç«¯: {len(client_embedding_gradients)}")

        # å…¨å±€è©•ä¼°
        avg_train_loss = sum(client_losses) / len(client_losses)
        avg_val_loss = sum(client_val_losses) / len(client_val_losses)

        print(f"\n  ã€æœ¬è¼ªçµæœã€‘")
        print(f"    å¹³å‡è¨“ç·´æå¤±: {avg_train_loss:.6f}")
        print(f"    å¹³å‡é©—è­‰æå¤±: {avg_val_loss:.6f}")

        # æ—©åœæª¢æŸ¥
        should_stop = server.evaluate_global(avg_train_loss, avg_val_loss, selected_clients)
        if should_stop:
            print(f"\næ—©åœè§¸ç™¼ï¼Œè¨“ç·´çµæŸæ–¼ç¬¬ {round_idx + 1} è¼ª")
            break

        # å®šæœŸè©•ä¼°
        if (round_idx + 1) % config.eval_interval == 0:
            print(f"\n  ã€è©•ä¼°æ‘˜è¦ - ç¬¬ {round_idx + 1} è¼ªã€‘")
            print(f"    æœ€ä½³é©—è­‰æå¤±: {server.best_val_loss:.6f}")
            print(f"    æ—©åœè¨ˆæ•¸: {server.patience_counter}/{config.early_stopping_patience}")

    # === æ­¥é©Ÿ 7: ä¿å­˜æ¨¡å‹ ===
    print(f"\n{'=' * 70}")
    print("ä¿å­˜æ¨¡å‹...")
    print(f"{'=' * 70}")

    server.save_final_model()

    # ä¿å­˜å®¢æˆ¶ç«¯ Fusion Models
    for client_name, client in clients.items():
        fusion_path = os.path.join(
            config.model_save_path,
            f"{client_name}_fusion_model.pth"
        )
        client.save_fusion_model(fusion_path)
        print(f"  âœ“ {client_name} Fusion Model å·²ä¿å­˜")

    # === æ­¥é©Ÿ 8: è¨“ç·´æ‘˜è¦ ===
    summary = server.get_training_summary()

    print(f"\n{'=' * 70}")
    print("è¨“ç·´å®Œæˆï¼")
    print(f"{'=' * 70}")
    print(f"\nè¨“ç·´æ‘˜è¦:")
    print(f"  - ç¸½è¼ªæ•¸: {summary['total_rounds']}")
    print(f"  - Weather Model æ›´æ–°æ¬¡æ•¸: {summary['weather_updates']}")
    print(f"  - å¯¦éš›é€šè¨Šç¯€çœ: {summary['comm_saving_actual']:.1f}%")
    print(f"  - æœ€ä½³é©—è­‰æå¤±: {summary['best_val_loss']:.6f}")
    print(f"  - æœ€çµ‚è¨“ç·´æå¤±: {summary['final_train_loss']:.6f}")
    print(f"  - æœ€çµ‚é©—è­‰æå¤±: {summary['final_val_loss']:.6f}")

    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {config.model_save_path}/")
    print("=" * 70)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='VFL Model Training')
    parser.add_argument('--config', default='config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾‘ (default: config.yaml)')
    args = parser.parse_args()

    train(args)
