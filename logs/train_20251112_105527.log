
======================================================================
VFL 垂直聯邦學習訓練 - FedAvg
======================================================================

配置摘要:
  - 演算法: vfl_fedavg
  - 全局輪數: 100
  - 客戶端數: 5
  - 批次大小: 32
  - 學習率: 0.001
  - 設備: mps

======================================================================
載入 Weather 數據 (雲端)
======================================================================
  - Weather 原始數據形狀: (13152, 10)
  - Weather 特徵數: 9
  ✓ Weather 標準化器已創建並保存
  ✓ Weather 數據已標準化: (13152, 9)

找到 5 個客戶端文件

Weather 序列已創建: (13056, 96, 9)

======================================================================
載入客戶端數據 (本地)
======================================================================

客戶端 [1/5]: Consumer_01
  ✓ 訓練樣本: 10368, 驗證樣本: 1296

客戶端 [2/5]: Consumer_02
  ✓ 訓練樣本: 10368, 驗證樣本: 1296

客戶端 [3/5]: Consumer_03
  ✓ 訓練樣本: 10368, 驗證樣本: 1296

客戶端 [4/5]: Consumer_04
  ✓ 訓練樣本: 10368, 驗證樣本: 1296

客戶端 [5/5]: Consumer_05
  ✓ 訓練樣本: 10368, 驗證樣本: 1296

總共載入 5 個客戶端

======================================================================
Per-FedAvg 個性化 HFL 模型初始化
======================================================================
  ⚠ Per-FedAvg 個性化功能需要先運行 PerFedAvgHVP 訓練
  → 當前使用隨機初始化的 HFL 模型

======================================================================
初始化 VFL Server 和 Clients
======================================================================
======================================================================
VFL Server 初始化
======================================================================

全局 Weather Model (雲端):
  - 特徵維度: 9
  - 模型維度: 256
  - 注意力頭數: 8
  - Transformer層數: 4
  - 總參數量: 3,195,649
  - 可訓練參數: 3,195,649
  - 設備: mps

訓練策略:
  - 總輪數: 100
  - 階段1 (10 輪): 每輪都訓練 Fusion + Weather
  - 階段2 (90 輪): 4 輪 Fusion，1 輪 Weather
  - 通訊節省: 72.0%
======================================================================

======================================================================
VFL Client 初始化: Consumer_01
======================================================================
  ⚠ 使用隨機初始化的 HFL 模型
  ✓ HFL Model 已凍結

HFL Model (本地，凍結):
  - 特徵維度: 14
  - 參數量: 3,196,929

Fusion Model (本地，可訓練):
  - Weather 嵌入維度: 256
  - HFL 嵌入維度: 256
  - 參數量: 165,635

設備: mps
======================================================================

======================================================================
VFL Client 初始化: Consumer_02
======================================================================
  ⚠ 使用隨機初始化的 HFL 模型
  ✓ HFL Model 已凍結

HFL Model (本地，凍結):
  - 特徵維度: 14
  - 參數量: 3,196,929

Fusion Model (本地，可訓練):
  - Weather 嵌入維度: 256
  - HFL 嵌入維度: 256
  - 參數量: 165,635

設備: mps
======================================================================

======================================================================
VFL Client 初始化: Consumer_03
======================================================================
  ⚠ 使用隨機初始化的 HFL 模型
  ✓ HFL Model 已凍結

HFL Model (本地，凍結):
  - 特徵維度: 14
  - 參數量: 3,196,929

Fusion Model (本地，可訓練):
  - Weather 嵌入維度: 256
  - HFL 嵌入維度: 256
  - 參數量: 165,635

設備: mps
======================================================================

======================================================================
VFL Client 初始化: Consumer_04
======================================================================
  ⚠ 使用隨機初始化的 HFL 模型
  ✓ HFL Model 已凍結

HFL Model (本地，凍結):
  - 特徵維度: 14
  - 參數量: 3,196,929

Fusion Model (本地，可訓練):
  - Weather 嵌入維度: 256
  - HFL 嵌入維度: 256
  - 參數量: 165,635

設備: mps
======================================================================

======================================================================
VFL Client 初始化: Consumer_05
======================================================================
  ⚠ 使用隨機初始化的 HFL 模型
  ✓ HFL Model 已凍結

HFL Model (本地，凍結):
  - 特徵維度: 14
  - 參數量: 3,196,929

Fusion Model (本地，可訓練):
  - Weather 嵌入維度: 256
  - HFL 嵌入維度: 256
  - 參數量: 165,635

設備: mps
======================================================================

======================================================================
開始聯邦學習訓練...
======================================================================

──────────────────────────────────────────────────────────────────────
聯邦學習輪次 [1/100]
──────────────────────────────────────────────────────────────────────
  訓練模式: Fusion Model + Weather Model ⚡

  選中客戶端: ['Consumer_05', 'Consumer_04', 'Consumer_03', 'Consumer_01', 'Consumer_02']

  Server 計算 Weather 嵌入向量:
Traceback (most recent call last):
  File "/Users/p304/Documents/Shen實驗室/第九次實驗/VFL/train.py", line 495, in <module>
    train(args)
  File "/Users/p304/Documents/Shen實驗室/第九次實驗/VFL/train.py", line 367, in train
    train_embeddings = server.compute_weather_embeddings(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/Documents/Shen實驗室/第九次實驗/VFL/Server.py", line 281, in compute_weather_embeddings
    embeddings = self.global_weather_model.forward_embedding(weather_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/Documents/Shen實驗室/第九次實驗/VFL/Model.py", line 230, in forward_embedding
    output = self._encode(x, src_mask)  # (batch_size, seq_len, d_model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/Documents/Shen實驗室/第九次實驗/VFL/Model.py", line 168, in _encode
    output = self.transformer(x, src_mask)  # (batch_size, seq_len, d_model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 916, in forward
    x = self.norm2(x + self._ff_block(x))
                       ^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 941, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
                                                  ^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/p304/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: MPS backend out of memory (MPS allocated: 57.15 GB, other allocations: 3.82 GB, max allowed: 63.65 GB). Tried to allocate 3.80 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
